"""
Module d·ª± ƒëo√°n gi√° s·ª≠ d·ª•ng LSTM Neural Network
"""
import numpy as np
import pandas as pd
import logging
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
import warnings

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


class LSTMPredictor:
    """M√¥ h√¨nh LSTM d·ª± ƒëo√°n gi√°"""
    
    def __init__(self, lookback=60):
        """
        Kh·ªüi t·∫°o predictor
        
        Args:
            lookback (int): S·ªë ng√†y ƒë·ªÉ xem x√©t
        """
        self.lookback = lookback
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.model = None
        self.history = None
    
    def prepare_data(self, df, test_size=0.2):
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh
        
        Args:
            df (pd.DataFrame): DataFrame v·ªõi c·ªôt Close
            test_size (float): T·ª∑ l·ªá test data
            
        Returns:
            tuple: (X_train, y_train, X_test, y_test)
        """
        # L·∫•y d·ªØ li·ªáu Close
        data = df['Close'].values.reshape(-1, 1)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaled_data = self.scaler.fit_transform(data)
        
        # T·∫°o sequence
        X, y = [], []
        for i in range(len(scaled_data) - self.lookback):
            X.append(scaled_data[i:i + self.lookback])
            y.append(scaled_data[i + self.lookback])
        
        X, y = np.array(X), np.array(y)
        
        # Chia train/test
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        logger.info(f"‚úì D·ªØ li·ªáu chu·∫©n b·ªã: {len(X_train)} train, {len(X_test)} test")
        
        return X_train, y_train, X_test, y_test
    
    def build_model(self, input_shape):
        """
        X√¢y d·ª±ng m√¥ h√¨nh LSTM
        
        Args:
            input_shape (tuple): H√¨nh d·∫°ng input (lookback, 1)
        """
        self.model = Sequential([
            LSTM(units=50, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(units=50, return_sequences=True),
            Dropout(0.2),
            LSTM(units=50),
            Dropout(0.2),
            Dense(units=25),
            Dense(units=1)
        ])
        
        self.model.compile(
            optimizer='adam',
            loss='mean_squared_error',
            metrics=['mae']
        )
        
        logger.info("‚úì M√¥ h√¨nh LSTM ƒë∆∞·ª£c x√¢y d·ª±ng")
    
    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.2):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh
        
        Args:
            X_train (np.array): D·ªØ li·ªáu train
            y_train (np.array): Target train
            epochs (int): S·ªë epoch
            batch_size (int): Batch size
            validation_split (float): T·ª∑ l·ªá validation
        """
        if self.model is None:
            self.build_model((X_train.shape[1], X_train.shape[2]))
        
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        
        self.history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=[early_stop],
            verbose=0
        )
        
        logger.info(f"‚úì M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán xong")
    
    def evaluate(self, X_test, y_test):
        """
        ƒê√°nh gi√° m√¥ h√¨nh
        
        Args:
            X_test (np.array): D·ªØ li·ªáu test
            y_test (np.array): Target test
            
        Returns:
            dict: C√°c metric ƒë√°nh gi√°
        """
        if self.model is None:
            return {}
        
        y_pred = self.model.predict(X_test, verbose=0)
        
        # Denormalize
        y_test_actual = self.scaler.inverse_transform(y_test)
        y_pred_actual = self.scaler.inverse_transform(y_pred)
        
        # T√≠nh metric
        mse = mean_squared_error(y_test_actual, y_pred_actual)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test_actual, y_pred_actual)
        r2 = r2_score(y_test_actual, y_pred_actual)
        
        metrics = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2
        }
        
        logger.info(f"‚úì ƒê√°nh gi√° m√¥ h√¨nh:")
        logger.info(f"  - RMSE: ${rmse:.2f}")
        logger.info(f"  - MAE: ${mae:.2f}")
        logger.info(f"  - R¬≤: {r2:.4f}")
        
        return metrics
    
    def predict_next(self, df, periods=7):
        """
        D·ª± ƒëo√°n gi√° cho c√°c ng√†y ti·∫øp theo
        
        Args:
            df (pd.DataFrame): DataFrame d·ªØ li·ªáu l·ªãch s·ª≠
            periods (int): S·ªë ng√†y d·ª± ƒëo√°n
            
        Returns:
            list: Gi√° d·ª± ƒëo√°n cho periods ng√†y
        """
        if self.model is None:
            logger.error("‚úó M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán")
            return []
        
        # L·∫•y d·ªØ li·ªáu cu·ªëi c√πng
        data = df['Close'].values.reshape(-1, 1)
        scaled_data = self.scaler.transform(data)
        
        # B·∫Øt ƒë·∫ßu t·ª´ d·ªØ li·ªáu cu·ªëi c√πng
        last_sequence = scaled_data[-self.lookback:].copy()
        
        predictions = []
        
        for _ in range(periods):
            # D·ª± ƒëo√°n
            next_pred = self.model.predict(
                last_sequence.reshape(1, self.lookback, 1),
                verbose=0
            )
            
            # L∆∞u d·ª± ƒëo√°n
            predictions.append(next_pred[0, 0])
            
            # C·∫≠p nh·∫≠t sequence
            last_sequence = np.append(last_sequence[1:], next_pred)
        
        # Denormalize
        predictions = np.array(predictions).reshape(-1, 1)
        predictions_actual = self.scaler.inverse_transform(predictions)
        
        return predictions_actual.flatten().tolist()
    
    def train_and_predict(self, df, periods=7):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh v√† d·ª± ƒëo√°n
        
        Args:
            df (pd.DataFrame): DataFrame d·ªØ li·ªáu
            periods (int): S·ªë ng√†y d·ª± ƒëo√°n
            
        Returns:
            tuple: (predictions, metrics)
        """
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        X_train, y_train, X_test, y_test = self.prepare_data(df)
        
        # X√¢y d·ª±ng v√† hu·∫•n luy·ªán
        self.build_model((X_train.shape[1], X_train.shape[2]))
        self.train(X_train, y_train, epochs=50)
        
        # ƒê√°nh gi√°
        metrics = self.evaluate(X_test, y_test)
        
        # D·ª± ƒëo√°n
        predictions = self.predict_next(df, periods)
        
        return predictions, metrics


if __name__ == "__main__":
    # Test
    from data_fetcher import DataFetcher
    
    print("=" * 50)
    print("TEST: D·ª± ƒëo√°n gi√° Bitcoin")
    print("=" * 50)
    
    btc_data = DataFetcher.fetch_crypto_data("bitcoin", days=365)
    
    predictor = LSTMPredictor(lookback=60)
    predictions, metrics = predictor.train_and_predict(btc_data, periods=7)
    
    print(f"\nüìà D·ª± ƒëo√°n gi√° Bitcoin 7 ng√†y t·ªõi:")
    current_price = btc_data['Close'].iloc[-1]
    for i, pred in enumerate(predictions, 1):
        change = ((pred - current_price) / current_price) * 100
        print(f"  Ng√†y {i}: ${pred:.2f} ({change:+.2f}%)")
