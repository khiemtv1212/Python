# HÆ°á»›ng dáº«n: Crawl dá»¯ liá»‡u + Táº£i hÃ¬nh áº£nh

## Tá»•ng quan

BÃ¢y giá» báº¡n cÃ³ thá»ƒ:
1. **Crawl dá»¯ liá»‡u web** (tiÃªu Ä‘á», liÃªn káº¿t, mÃ´ táº£)
2. **Táº£i hÃ¬nh áº£nh tá»± Ä‘á»™ng** (poster, thumbnail, áº£nh chi tiáº¿t)
3. **LÆ°u cáº£ dá»¯ liá»‡u vÃ  hÃ¬nh áº£nh** vÃ o folder

## 3 CÃ¡ch sá»­ dá»¥ng

### 1ï¸âƒ£ Crawl PhimHay vá»›i hÃ¬nh áº£nh

```python
from crawl_with_images import crawl_phimhay_with_images

# Crawl 2 trang, táº£i tá»‘i Ä‘a 10 hÃ¬nh
crawl_phimhay_with_images(pages=2, max_images=10, detail_crawl=True)
```

**Káº¿t quáº£:**
- `data/phimhay_with_images.json` - Dá»¯ liá»‡u + Ä‘Æ°á»ng dáº«n hÃ¬nh áº£nh
- `downloads/phimhay/list_images/` - HÃ¬nh poster tá»« trang danh sÃ¡ch
- `downloads/phimhay/detail_images/` - HÃ¬nh tá»« trang chi tiáº¿t

**JSON output:**
```json
[
  {
    "title": "Doraemon Movie",
    "url": "https://phimhay.co.in/...",
    "image": "https://...",
    "image_local": "downloads/phimhay/list_images/doraemon.jpg",
    "detail_image_local": "downloads/phimhay/detail_images/Doraemon Movie.jpg",
    "rating": "8.5",
    "year": "2024"
  }
]
```

---

### 2ï¸âƒ£ Crawl AnimeHay vá»›i hÃ¬nh áº£nh + episode

```python
from crawl_with_images import crawl_animehay_with_images

# Crawl anime, táº£i poster + thumbnail episode
crawl_animehay_with_images(category='anime-1', pages=2, max_images=5)
```

**Káº¿t quáº£:**
- `data/animehay_with_images.json` - Anime + episodes + paths hÃ¬nh
- `downloads/animehay/anime_posters/` - Poster anime
- `downloads/animehay/episode_thumbnails/` - Thumbnail episode

**JSON output:**
```json
[
  {
    "title": "Demon Slayer",
    "url": "https://animehay.life/...",
    "image": "https://...",
    "image_local": "downloads/animehay/anime_posters/...",
    "year": "2024",
    "episodes": [
      {
        "episode_number": 1,
        "title": "Ep 1",
        "image": "https://...",
        "image_local": "downloads/animehay/episode_thumbnails/..."
      }
    ]
  }
]
```

---

### 3ï¸âƒ£ Crawl website tÃ¹y chá»‰nh + hÃ¬nh áº£nh

```python
from crawl_with_images import crawl_custom_website_with_images

# DÃ¹ng config 'phimhay' (hoáº·c website khÃ¡c)
crawl_custom_website_with_images('phimhay', pages=2, image_field='image')
```

---

## ğŸš€ PHÆ¯Æ NG PHÃP BATCH: Táº£i nhiá»u hÃ¬nh cÃ¹ng lÃºc (NHANH NHáº¤T!)

### ğŸ“Œ Method 1: Táº£i song song (Parallel) - PHIMHAY

```python
from batch_download import batch_download_phimhay_parallel

# Táº£i 100 hÃ¬nh cÃ¹ng 1 lÃºc vÃ o 1 folder!
items, result = batch_download_phimhay_parallel(
    pages=2,          # Crawl 2 trang
    max_workers=8     # Táº£i 8 cÃ¡i hÃ¬nh cÃ¹ng lÃºc
)

print(f"Downloaded: {result['success_count']} images")
print(f"Total size: {result['total_size_mb']} MB")
```

**Output:**
```
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 50/50 images | success: 50, failed: 0

âœ“ Success: 50/50
âœ— Failed: 0/50
ğŸ“Š Total size: 125.5 MB
```

### ğŸ“Œ Method 2: Táº£i song song - ANIMEHAY

```python
from batch_download import batch_download_animehay_parallel

# Táº£i poster anime + episode thumbnails cÃ¹ng lÃºc!
animes, result = batch_download_animehay_parallel(
    category='anime-1',
    pages=2,
    max_workers=6     # 6 files cÃ¹ng lÃºc
)

print(f"Downloaded: {result['total_size_mb']} MB")
```

### ğŸ“Œ Method 3: Táº£i báº¥t ká»³ website nÃ o

```python
from batch_download import batch_download_website

# Crawl + táº£i hÃ¬nh cho website config báº¥t ká»³
items, result = batch_download_website(
    config_name='phimhay',
    pages=2,
    max_workers=4
)
```

### ğŸ“Œ Method 4: Táº¢I SIÃŠU NHANH - Chá»‰ tá»« URLs (CHO Táº¤T Cáº¢)

```python
from batch_download import batch_download_from_urls

# Táº£i 1000 hÃ¬nh chá»‰ tá»« danh sÃ¡ch URLs!
urls = [
    'https://example.com/img1.jpg',
    'https://example.com/img2.jpg',
    'https://example.com/img3.jpg',
    # ... thÃªm táº¥t cáº£ URLs
]

result = batch_download_from_urls(
    urls,
    folder_name='my_images',
    max_workers=10  # 10 hÃ¬nh cÃ¹ng lÃºc!
)

# Káº¿t quáº£
print(result['success_count'])    # Táº£i Ä‘Æ°á»£c bao nhiÃªu
print(result['total_size_mb'])    # Tá»•ng size
```

---



## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c hÃ¬nh áº£nh

```
downloads/
â”œâ”€â”€ phimhay/
â”‚   â”œâ”€â”€ list_images/     # HÃ¬nh tá»« danh sÃ¡ch
â”‚   â”‚   â”œâ”€â”€ movie1.jpg
â”‚   â”‚   â”œâ”€â”€ movie2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ detail_images/   # HÃ¬nh tá»« trang chi tiáº¿t
â”‚       â””â”€â”€ ...
â”œâ”€â”€ animehay/
â”‚   â”œâ”€â”€ anime_posters/   # Poster anime
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ episode_thumbnails/  # Thumbnail episode
â”‚       â””â”€â”€ ...
â””â”€â”€ [config_name]/
    â””â”€â”€ items/
        â””â”€â”€ ...
```

---

## ğŸ”§ ImageDownloader - Chi tiáº¿t

### Khá»Ÿi táº¡o downloader

```python
from utils.image_downloader import ImageDownloader

# CÆ¡ báº£n
downloader = ImageDownloader()
# â†’ LÆ°u vÃ o: downloads/images/
# â†’ max_workers: 4 (máº·c Ä‘á»‹nh)

# Custom folder + workers
downloader = ImageDownloader(
    base_dir='my_images/phimhay',
    timeout=15,      # 15s timeout per image
    delay=1,         # 1s delay between single downloads
    max_workers=8    # 8 hÃ¬nh táº£i song song
)
```

### Táº£i 1 hÃ¬nh áº£nh

```python
result = downloader.download_image(
    url='https://example.com/movie.jpg',
    subfolder='movies',
    filename='movie_2024.jpg'
)

print(result)
# {
#     'success': True,
#     'url': 'https://...',
#     'local_path': 'downloads/images/movies/movie_2024.jpg',
#     'error': None,
#     'file_size': 45000  # bytes
# }
```

### Táº£i nhiá»u hÃ¬nh SEQUENTIAL (Tá»«ng cÃ¡i má»™t)

```python
images_urls = [
    'https://example.com/img1.jpg',
    'https://example.com/img2.jpg',
    'https://example.com/img3.jpg'
]

results = downloader.download_images_batch(
    images_urls,
    subfolder='batch'
)

for result in results:
    print(f"{result['url']}: {result['success']}")
```

### âš¡ Táº£i nhiá»u hÃ¬nh PARALLEL (CÃ™NG LÃšC - NHANH!)

```python
image_urls = [
    'https://example.com/img1.jpg',
    'https://example.com/img2.jpg',
    # ... 1000+ hÃ¬nh khÃ¡c
]

# Táº¢I Táº¤T Cáº¢ CÃ™NG LÃšC!
result = downloader.download_images_parallel(
    image_urls,
    subfolder='fast_batch',
    max_workers=8,      # 8 hÃ¬nh cÃ¹ng lÃºc
    show_progress=True  # Hiá»‡n progress bar
)

print(result)
# {
#     'success_count': 995,
#     'failed_count': 5,
#     'total_attempted': 1000,
#     'total_size_mb': 512.5
# }
```

### Helper: Extract + Download tá»« items

```python
from utils.image_downloader import extract_and_download_images

# Báº¡n Ä‘Ã£ crawl items nÃ y
items = [
    {'title': 'Movie 1', 'image': 'https://...', ...},
    {'title': 'Movie 2', 'image': 'https://...', ...},
]

# Extract hÃ¬nh + táº£i
result = extract_and_download_images(
    items=items,
    image_field='image',        # TrÆ°á»ng chá»©a URL
    subfolder='my_movies',
    max_images=10              # Táº£i tá»‘i Ä‘a 10 cÃ¡i
)

# CÃ¡c items giá» cÃ³ field má»›i
print(items[0]['image_local'])  # Path hÃ¬nh Ä‘Ã£ táº£i
print(f"Success: {result['success_count']}")
print(f"Failed: {result['failed_count']}")
```

---

## ğŸ“Š Kiá»ƒm tra thá»‘ng kÃª

```python
stats = downloader.get_download_stats()
print(stats)

# Output:
# {
#     'total_files': 42,
#     'total_size_mb': 125.5,
#     'base_dir': 'downloads/images'
# }
```

---

## ğŸš€ VÃ­ dá»¥ thá»±c táº¿

### Crawl + save + kiá»ƒm tra

```python
from crawl_with_images import crawl_phimhay_with_images
from utils.image_downloader import ImageDownloader

# 1. Crawl dá»¯ liá»‡u + táº£i hÃ¬nh
crawl_phimhay_with_images(
    pages=3,           # 3 trang
    max_images=50,     # Tá»‘i Ä‘a 50 hÃ¬nh
    detail_crawl=True  # CÅ©ng crawl trang chi tiáº¿t
)

# 2. Kiá»ƒm tra káº¿t quáº£
import json
with open('data/phimhay_with_images.json') as f:
    data = json.load(f)
    
print(f"Total items: {len(data)}")
print(f"Total images: {sum(1 for item in data if item.get('image_local'))}")

# 3. Kiá»ƒm tra size
downloader = ImageDownloader('downloads/phimhay')
stats = downloader.get_download_stats()
print(f"Downloaded: {stats['total_files']} images, {stats['total_size_mb']} MB")
```

---

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Custom headers & cookies

```python
from crawlers.flexible_crawler import FlexibleWebCrawler
from config.crawler_config import ConfigManager
from utils.image_downloader import ImageDownloader

# Load config
config_mgr = ConfigManager()
config = config_mgr.load_config('phimhay')

# Modify headers for images
config.headers['Referer'] = 'https://phimhay.co.in/'

crawler = FlexibleWebCrawler(config)
items = crawler.crawl_items()

# Download with same headers
downloader = ImageDownloader('downloads/phimhay')
results = downloader.download_images_batch(
    [item['image'] for item in items],
    subfolder='images'
)
```

### Timeout cho trang cháº­m

```python
# TÄƒng timeout cho website cháº­m
downloader = ImageDownloader(
    base_dir='downloads/slow_site',
    timeout=30  # 30 giÃ¢y
)
```

### Delay Ä‘á»ƒ respects website

```python
# Delay 2 giÃ¢y giá»¯a má»—i download (Ä‘áº¹p trai!)
downloader = ImageDownloader(
    base_dir='downloads/images',
    delay=2  # Tháº¯ng nhÃ¢n loáº¡i
)
```

---

## âš ï¸ Xá»­ lÃ½ lá»—i

```python
result = downloader.download_image(url)

if not result['success']:
    print(f"Error: {result['error']}")
    # CÃ¡c lá»—i cÃ³ thá»ƒ:
    # - "Invalid URL"
    # - "Connection timeout"
    # - "404 Not Found"
    # - "403 Forbidden"
    # - etc...
else:
    print(f"Saved to: {result['local_path']}")
    print(f"Size: {result['file_size']} bytes")
```

---

## ğŸ¯ Tá»•ng káº¿t

| TÃ¡c vá»¥ | Code | Tá»‘c Ä‘á»™ |
|--------|------|--------|
| **Crawl + download phimhay** | `crawl_phimhay_with_images()` | Normal |
| **Crawl + download anime** | `crawl_animehay_with_images()` | Normal |
| **Batch: Parallel phimhay** | `batch_download_phimhay_parallel()` | âš¡ NHANH |
| **Batch: Parallel animehay** | `batch_download_animehay_parallel()` | âš¡ NHANH |
| **Batch: Parallel website** | `batch_download_website()` | âš¡ NHANH |
| **Batch: Tá»« URLs** | `batch_download_from_urls()` | âš¡ SIÃŠU NHANH |
| **Download 1 hÃ¬nh** | `downloader.download_image()` | Normal |
| **Download nhiá»u (sequential)** | `downloader.download_images_batch()` | Normal |
| **Download nhiá»u (parallel)** | `downloader.download_images_parallel()` | âš¡ NHANH |

---

## ğŸ“ Notes

1. **HÃ¬nh Ä‘Ã£ táº£i sáº½ bá»‹ bá» qua** - KhÃ´ng táº£i láº¡i náº¿u file Ä‘Ã£ tá»“n táº¡i
2. **User-Agent tá»± Ä‘á»™ng** - TrÃ¡nh Ä‘Æ°á»£c má»™t sá»‘ website cháº·n
3. **Delay giá»¯a downloads** - Respects website, khÃ´ng spam request
4. **Timeout máº·c Ä‘á»‹nh 10s** - Äiá»u chá»‰nh náº¿u cáº§n
5. **Lá»—i khÃ´ng dá»«ng process** - Cá»© tiáº¿p tá»¥c táº£i cÃ¡c hÃ¬nh cÃ²n láº¡i

---

## ğŸ”— File liÃªn quan

- `utils/image_downloader.py` - Core ImageDownloader class
- `crawl_with_images.py` - CÃ¡c vÃ­ dá»¥ sáºµn dÃ¹ng
- `data/` - Output JSON files
- `downloads/` - ThÆ° má»¥c lÆ°u hÃ¬nh áº£nh
